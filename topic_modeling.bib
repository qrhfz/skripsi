
@inproceedings{qomariyah_topic_2019,
	address = {Surakarta, Indonesia},
	title = {Topic modeling {Twitter} data using {Latent} {Dirichlet} {Allocation} and {Latent} {Semantic} {Analysis}},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.5139825},
	doi = {10.1063/1.5139825},
	abstract = {The industrial world has entered the era of industrial revolution 4.0. In this era, there is an urgent data requirement from the community to support service policies. Because of that, Surabaya Government made Media Center Surabaya. This media is used to accommodate all the aspiration of Surabaya citizen. To access this media, a citizen can use Twitter. The topic which is discussed in Twitter is important information that we need to know. The information can be used to improve the performance of Surabaya Government services. Twitter data is a text data that consists of thousands of variables. Text mining is frequently used to analyze this kind of data, including topic modeling and sentiment analysis. This study would work on topic modeling focused on the algorithm employing Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA). The evaluation of the algorithm performance uses the topic coherence. As unstructured data, the Twitter data need preprocessing before the analysis. The stages of preprocessing include cleansing, stemming, and stop words. The advantages of LSA are fast and easy to implement. LSA, on the other hand, doesn’t consider the relationship between documents in the corpus, while LDA does. This study shows that LDA gives a better result than LSA.},
	language = {en},
	urldate = {2022-09-11},
	author = {Qomariyah, Siti and Iriawan, Nur and Fithriasari, Kartika},
	year = {2019},
	pages = {020093},
	file = {Qomariyah et al. - 2019 - Topic modeling Twitter data using Latent Dirichlet.pdf:/home/q/Zotero/storage/YAV7WY9K/Qomariyah et al. - 2019 - Topic modeling Twitter data using Latent Dirichlet.pdf:application/pdf},
}

@inproceedings{surjandari_mining_2018,
	address = {Zhengzhou},
	title = {Mining {Web} {Log} {Data} for {News} {Topic} {Modeling} {Using} {Latent} {Dirichlet} {Allocation}},
	isbn = {978-1-5386-5500-9},
	url = {https://ieeexplore.ieee.org/document/8612574/},
	doi = {10.1109/ICISCE.2018.00076},
	abstract = {The growth of e-news platforms, the most popular and accessible media for sharing information, has resulted in the increase of digital news articles volume. Users’ navigation across news articles in e-news platform, which is captured in form of web log data, is able to show which articles are read by users. News articles read by users can illustrate topics of interest and public unrest towards a particular event, field, or aspect. The knowledge and understanding of topics of interest and public unrest are important, especially for subsequent newsletter journalists and government in policy-making. This study was conducted in response to the importance of extracting topics from news articles read by users or public. Latent dirichlet allocation was used as topic modeling algorithm from list of news article title and category obtained from user web log data across 5 e-news publisher domains in Indonesia. The topic modeling process results in 12 topics of news articles. The results of this study provide insight to enews platform regarding the reading material focus of users.},
	language = {en},
	urldate = {2022-09-11},
	booktitle = {2018 5th {International} {Conference} on {Information} {Science} and {Control} {Engineering} ({ICISCE})},
	publisher = {IEEE},
	author = {Surjandari, Isti and Rosyidah, Asma and Zulkarnain, Zulkarnain and Laoh, Enrico},
	month = jul,
	year = {2018},
	pages = {331--335},
	file = {Surjandari et al. - 2018 - Mining Web Log Data for News Topic Modeling Using .pdf:/home/q/Zotero/storage/YZT6K2M7/Surjandari et al. - 2018 - Mining Web Log Data for News Topic Modeling Using .pdf:application/pdf},
}

@inproceedings{zhang_enhancing_2020,
	address = {Fuzhou, China},
	title = {Enhancing {Short} {Text} {Topic} {Modeling} with {FastText} {Embeddings}},
	isbn = {978-1-72816-499-1},
	url = {https://ieeexplore.ieee.org/document/9196423/},
	doi = {10.1109/ICBAIE49996.2020.00060},
	urldate = {2022-09-18},
	booktitle = {2020 {International} {Conference} on {Big} {Data}, {Artificial} {Intelligence} and {Internet} of {Things} {Engineering} ({ICBAIE})},
	publisher = {IEEE},
	author = {Zhang, Fan and Gao, Wang and Fang, Yuan and Zhang, Bo},
	month = jun,
	year = {2020},
	pages = {255--259},
	file = {Zhang et al. - 2020 - Enhancing Short Text Topic Modeling with FastText .pdf:/home/q/Zotero/storage/FPC82LLK/Zhang et al. - 2020 - Enhancing Short Text Topic Modeling with FastText .pdf:application/pdf},
}

@article{yang_language_2019,
	title = {Language {Model}-{Driven} {Topic} {Clustering} and {Summarization} for {News} {Articles}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8936376/},
	doi = {10.1109/ACCESS.2019.2960538},
	abstract = {Topic models have been widely utilized in Topic Detection and Tracking tasks, which aim to detect, track, and describe topics from a stream of broadcast news reports. However, most existing topic models neglect semantic or syntactic information and lack readable topic descriptions. To exploit semantic and syntactic information, Language Models (LMs) have been applied in many supervised NLP tasks. However, there are still no extensions of LMs for unsupervised topic clustering. Moreover, it is difﬁcult to employ general LMs (e.g., BERT) to produce readable topic summaries due to the mismatch between the pretraining method and the summarization task. In this paper, noticing the similarity between content and summary, ﬁrst we propose a Language Model-based Topic Model (LMTM) for Topic Clustering by using an LM to generate a deep contextualized word representation. Then, a new method of training a Topic Summarization Model is introduced, where it is not only able to produce brief topic summaries but also used as an LM in LMTM for topic clustering. Empirical evaluations of two different datasets show that the proposed LMTM method achieves better performance over four baselines for JC, FMI, precision, recall and F1-score. Additionally, the generated readable and reasonable summaries also validate the rationality of our model components.},
	language = {en},
	urldate = {2022-09-18},
	journal = {IEEE Access},
	author = {Yang, Peng and Li, Wenhan and Zhao, Guangzhen},
	year = {2019},
	pages = {185506--185519},
	file = {Yang et al. - 2019 - Language Model-Driven Topic Clustering and Summari.pdf:/home/q/Zotero/storage/WY5KHTUC/Yang et al. - 2019 - Language Model-Driven Topic Clustering and Summari.pdf:application/pdf},
}

@article{liu_collaboratively_2020,
	title = {Collaboratively {Modeling} and {Embedding} of {Latent} {Topics} for {Short} {Texts}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9102242/},
	doi = {10.1109/ACCESS.2020.2997973},
	abstract = {Deriving a successful document representation is the critical challenge in many downstream tasks in NLP, especially when documents are very short. It is challenging to handle the sparsity and the noise problems confronting short texts. Some approaches employ latent topic models, based on global word co-occurrence, to obtain topic distribution as the representation. Others leverage word embeddings, which consider local conditional dependencies, to map a document as a summation vector of them. Unlike the existing works which explore the strategy of utilizing one to help the other, i.e., topic models for word embeddings or vice versa, we propose CME-DMM, a collaboratively modeling and embedding framework for capturing coherent latent topics from short texts. CME-DMM incorporates topic and word embeddings through the attention mechanism and implants them into the latent topic models, which signiﬁcantly improve the quality of latent topics. Extensive experiments demonstrate that CME-DMM could perceive more coherent topics than other popular methods, resulting in a better performance in downstream NLP tasks such as classiﬁcation. Besides the interpretable latent topics, the corresponding topic embeddings can describe the meanings of latent topics in the semantic space. The attention vectors, as a by-product of the learning process, can identify the keywords in noisy short texts.},
	language = {en},
	urldate = {2022-09-18},
	journal = {IEEE Access},
	author = {Liu, Zheng and Qin, Tingting and Chen, Ke-Jia and Li, Yun},
	year = {2020},
	pages = {99141--99153},
	file = {Liu et al. - 2020 - Collaboratively Modeling and Embedding of Latent T.pdf:/home/q/Zotero/storage/ZKG6FDTC/Liu et al. - 2020 - Collaboratively Modeling and Embedding of Latent T.pdf:application/pdf},
}

@article{qiang_short_2022,
	title = {Short {Text} {Topic} {Modeling} {Techniques}, {Applications}, and {Performance}: {A} {Survey}},
	volume = {34},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Short {Text} {Topic} {Modeling} {Techniques}, {Applications}, and {Performance}},
	url = {https://ieeexplore.ieee.org/document/9086136/},
	doi = {10.1109/TKDE.2020.2992485},
	abstract = {Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the ﬁrst comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a uniﬁed interface, benchmark datasets, to facilitate the expansion of new methods in this research ﬁeld. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.},
	language = {en},
	number = {3},
	urldate = {2022-09-18},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Qiang, Jipeng and Qian, Zhenyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
	month = mar,
	year = {2022},
	pages = {1427--1445},
	file = {Qiang et al. - 2022 - Short Text Topic Modeling Techniques, Applications.pdf:/home/q/Zotero/storage/US8H6F34/Qiang et al. - 2022 - Short Text Topic Modeling Techniques, Applications.pdf:application/pdf},
}

@article{yi_topic_2020,
	title = {Topic {Modeling} for {Short} {Texts} via {Word} {Embedding} and {Document} {Correlation}},
	volume = {8},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8993771/},
	doi = {10.1109/ACCESS.2020.2973207},
	abstract = {Topic modeling is a widely studied foundational and interesting problem in the text mining domains. Conventional topic models based on word co-occurrences infer the hidden semantic structure from a corpus of documents. However, due to the limited length of short text, data sparsity impedes the inference process of conventional topic models and causes unsatisfactory results on short texts. In fact, each short text usually contains a limited number of topics, and understanding semantic content of short text needs to the relevant background knowledge. Inspired by the observed information, we propose a regularized non-negative matrix factorization topic model for short texts, named TRNMF. The proposed model leverages pre-trained distributional vector representation of words to overcome the data sparsity problem of short texts. Meanwhile, the method employs the clustering mechanism under document-totopic distributions during the topic inference by using Gibbs Sampling Dirichlet Multinomial Mixture model. TRNMF integrates successfully both word co-occurrence regularization and sentence similarity regularization into topic modeling for short texts. Through extensive experiments on constructed real-world short text corpus, experimental results show that TRNMF can achieve better results than the state-of-the-art methods in term of topic coherence measure and text classiﬁcation task.},
	language = {en},
	urldate = {2022-09-18},
	journal = {IEEE Access},
	author = {Yi, Feng and Jiang, Bo and Wu, Jianjun},
	year = {2020},
	pages = {30692--30705},
	file = {Yi et al. - 2020 - Topic Modeling for Short Texts via Word Embedding .pdf:/home/q/Zotero/storage/YNCFKADC/Yi et al. - 2020 - Topic Modeling for Short Texts via Word Embedding .pdf:application/pdf},
}
